{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(units=1, input_shape=[1])\n",
    "])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f14ec0f7378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f14ec0f7378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "16/16 [==============================] - 3s 204ms/sample - loss: 6565.4722\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 0s 469us/sample - loss: 5147.9868\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 0s 455us/sample - loss: 4036.5413\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 0s 399us/sample - loss: 3165.0618\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 0s 260us/sample - loss: 2481.7380\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 0s 569us/sample - loss: 1945.9465\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 0s 142us/sample - loss: 1525.8350\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 0s 151us/sample - loss: 1196.4268\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 0s 101us/sample - loss: 938.1390\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 0s 116us/sample - loss: 735.6163\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 0s 135us/sample - loss: 576.8187\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 0s 118us/sample - loss: 452.3058\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 0s 112us/sample - loss: 354.6754\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 0s 174us/sample - loss: 278.1236\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 0s 176us/sample - loss: 218.0993\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 0s 227us/sample - loss: 171.0346\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 0s 193us/sample - loss: 134.1309\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 0s 106us/sample - loss: 105.1948\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 0s 115us/sample - loss: 82.5059\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 0s 100us/sample - loss: 64.7154\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 0s 196us/sample - loss: 50.7658\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 0s 86us/sample - loss: 39.8277\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 0s 108us/sample - loss: 31.2510\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 0s 149us/sample - loss: 24.5259\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 0s 91us/sample - loss: 19.2525\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 0s 143us/sample - loss: 15.1176\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 0s 221us/sample - loss: 11.8752\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 0s 112us/sample - loss: 9.3326\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 0s 118us/sample - loss: 7.3388\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 0s 187us/sample - loss: 5.7752\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 0s 292us/sample - loss: 4.5491\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 0s 161us/sample - loss: 3.5875\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 0s 218us/sample - loss: 2.8333\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 0s 403us/sample - loss: 2.2418\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 0s 139us/sample - loss: 1.7778\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 0s 134us/sample - loss: 1.4138\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 0s 104us/sample - loss: 1.1282\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 0s 138us/sample - loss: 0.9041\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 0s 204us/sample - loss: 0.7282\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 0s 151us/sample - loss: 0.5901\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 0s 117us/sample - loss: 0.4817\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 0s 108us/sample - loss: 0.3965\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 0s 118us/sample - loss: 0.3295\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 0s 132us/sample - loss: 0.2768\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 0s 123us/sample - loss: 0.2353\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 0s 157us/sample - loss: 0.2027\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 0s 167us/sample - loss: 0.1769\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 0s 127us/sample - loss: 0.1565\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 0s 157us/sample - loss: 0.1403\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 0s 159us/sample - loss: 0.1275\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 0s 109us/sample - loss: 0.1173\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 0s 101us/sample - loss: 0.1091\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 0s 246us/sample - loss: 0.1026\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 0s 215us/sample - loss: 0.0973\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 0s 230us/sample - loss: 0.0930\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 0s 138us/sample - loss: 0.0895\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 0s 278us/sample - loss: 0.0866\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 0s 139us/sample - loss: 0.0841\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 0s 114us/sample - loss: 0.0821\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 0s 144us/sample - loss: 0.0803\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 0s 78us/sample - loss: 0.0788\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 0s 241us/sample - loss: 0.0775\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 0s 187us/sample - loss: 0.0763\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 0s 165us/sample - loss: 0.0752\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 0s 146us/sample - loss: 0.0742\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 0s 105us/sample - loss: 0.0733\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 0s 111us/sample - loss: 0.0725\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 0s 105us/sample - loss: 0.0717\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 0s 97us/sample - loss: 0.0709\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 0s 239us/sample - loss: 0.0702\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 0s 172us/sample - loss: 0.0695\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 0s 194us/sample - loss: 0.0688\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 0s 378us/sample - loss: 0.0682\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 0s 136us/sample - loss: 0.0675\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 0s 272us/sample - loss: 0.0669\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 0s 154us/sample - loss: 0.0663\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 0s 235us/sample - loss: 0.0657\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 0s 312us/sample - loss: 0.0651\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 0s 227us/sample - loss: 0.0645\n",
      "Epoch 80/150\n",
      "16/16 [==============================] - 0s 272us/sample - loss: 0.0639\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 0s 152us/sample - loss: 0.0633\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 0s 143us/sample - loss: 0.0627\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 0s 172us/sample - loss: 0.0621\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 0s 162us/sample - loss: 0.0616\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 0s 247us/sample - loss: 0.0610\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 0s 128us/sample - loss: 0.0605\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 0s 287us/sample - loss: 0.0599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/150\n",
      "16/16 [==============================] - 0s 242us/sample - loss: 0.0594\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 0s 122us/sample - loss: 0.0589\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 0s 144us/sample - loss: 0.0583\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 0s 351us/sample - loss: 0.0578\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 0s 140us/sample - loss: 0.0573\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 0s 126us/sample - loss: 0.0568\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 0s 116us/sample - loss: 0.0563\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 0s 112us/sample - loss: 0.0557\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 0s 175us/sample - loss: 0.0552\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 0s 113us/sample - loss: 0.0548\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 0s 110us/sample - loss: 0.0543\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 0s 141us/sample - loss: 0.0538\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 0s 127us/sample - loss: 0.0533\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 0s 124us/sample - loss: 0.0528\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 0s 104us/sample - loss: 0.0523\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 0s 115us/sample - loss: 0.0519\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 0s 152us/sample - loss: 0.0514\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 0s 119us/sample - loss: 0.0509\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 0s 97us/sample - loss: 0.0505\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 0s 393us/sample - loss: 0.0500\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 0s 183us/sample - loss: 0.0496\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 0s 114us/sample - loss: 0.0491\n",
      "Epoch 110/150\n",
      "16/16 [==============================] - 0s 107us/sample - loss: 0.0487\n",
      "Epoch 111/150\n",
      "16/16 [==============================] - 0s 93us/sample - loss: 0.0482\n",
      "Epoch 112/150\n",
      "16/16 [==============================] - 0s 260us/sample - loss: 0.0478\n",
      "Epoch 113/150\n",
      "16/16 [==============================] - 0s 107us/sample - loss: 0.0474\n",
      "Epoch 114/150\n",
      "16/16 [==============================] - 0s 94us/sample - loss: 0.0470\n",
      "Epoch 115/150\n",
      "16/16 [==============================] - 0s 131us/sample - loss: 0.0465\n",
      "Epoch 116/150\n",
      "16/16 [==============================] - 0s 107us/sample - loss: 0.0461\n",
      "Epoch 117/150\n",
      "16/16 [==============================] - 0s 110us/sample - loss: 0.0457\n",
      "Epoch 118/150\n",
      "16/16 [==============================] - 0s 126us/sample - loss: 0.0453\n",
      "Epoch 119/150\n",
      "16/16 [==============================] - 0s 229us/sample - loss: 0.0449\n",
      "Epoch 120/150\n",
      "16/16 [==============================] - 0s 130us/sample - loss: 0.0445\n",
      "Epoch 121/150\n",
      "16/16 [==============================] - 0s 153us/sample - loss: 0.0441\n",
      "Epoch 122/150\n",
      "16/16 [==============================] - 0s 114us/sample - loss: 0.0437\n",
      "Epoch 123/150\n",
      "16/16 [==============================] - 0s 135us/sample - loss: 0.0433\n",
      "Epoch 124/150\n",
      "16/16 [==============================] - 0s 119us/sample - loss: 0.0429\n",
      "Epoch 125/150\n",
      "16/16 [==============================] - 0s 209us/sample - loss: 0.0425\n",
      "Epoch 126/150\n",
      "16/16 [==============================] - 0s 144us/sample - loss: 0.0421\n",
      "Epoch 127/150\n",
      "16/16 [==============================] - 0s 139us/sample - loss: 0.0417\n",
      "Epoch 128/150\n",
      "16/16 [==============================] - 0s 143us/sample - loss: 0.0414\n",
      "Epoch 129/150\n",
      "16/16 [==============================] - 0s 133us/sample - loss: 0.0410\n",
      "Epoch 130/150\n",
      "16/16 [==============================] - 0s 115us/sample - loss: 0.0406\n",
      "Epoch 131/150\n",
      "16/16 [==============================] - 0s 141us/sample - loss: 0.0403\n",
      "Epoch 132/150\n",
      "16/16 [==============================] - 0s 117us/sample - loss: 0.0399\n",
      "Epoch 133/150\n",
      "16/16 [==============================] - 0s 166us/sample - loss: 0.0395\n",
      "Epoch 134/150\n",
      "16/16 [==============================] - 0s 99us/sample - loss: 0.0392\n",
      "Epoch 135/150\n",
      "16/16 [==============================] - 0s 257us/sample - loss: 0.0388\n",
      "Epoch 136/150\n",
      "16/16 [==============================] - 0s 176us/sample - loss: 0.0385\n",
      "Epoch 137/150\n",
      "16/16 [==============================] - 0s 114us/sample - loss: 0.0381\n",
      "Epoch 138/150\n",
      "16/16 [==============================] - 0s 201us/sample - loss: 0.0378\n",
      "Epoch 139/150\n",
      "16/16 [==============================] - 0s 120us/sample - loss: 0.0375\n",
      "Epoch 140/150\n",
      "16/16 [==============================] - 0s 129us/sample - loss: 0.0371\n",
      "Epoch 141/150\n",
      "16/16 [==============================] - 0s 120us/sample - loss: 0.0368\n",
      "Epoch 142/150\n",
      "16/16 [==============================] - 0s 114us/sample - loss: 0.0365\n",
      "Epoch 143/150\n",
      "16/16 [==============================] - 0s 111us/sample - loss: 0.0361\n",
      "Epoch 144/150\n",
      "16/16 [==============================] - 0s 122us/sample - loss: 0.0358\n",
      "Epoch 145/150\n",
      "16/16 [==============================] - 0s 122us/sample - loss: 0.0355\n",
      "Epoch 146/150\n",
      "16/16 [==============================] - 0s 183us/sample - loss: 0.0352\n",
      "Epoch 147/150\n",
      "16/16 [==============================] - 0s 144us/sample - loss: 0.0348\n",
      "Epoch 148/150\n",
      "16/16 [==============================] - 0s 153us/sample - loss: 0.0345\n",
      "Epoch 149/150\n",
      "16/16 [==============================] - 0s 160us/sample - loss: 0.0342\n",
      "Epoch 150/150\n",
      "16/16 [==============================] - 0s 122us/sample - loss: 0.0339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f14f1e07f60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.arange(1.0,17.0,1.0,dtype=np.float32)\n",
    "y=np.arange(10.0,170.0,10.0,dtype=np.float32)\n",
    "\n",
    "model.fit(x, y, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (16,)\n",
      "strides:  (4,)\n",
      "itemsize:  4\n",
      "aligned:  True\n",
      "contiguous:  True\n",
      "fortran:  True\n",
      "data pointer: 0x558310d7e800\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float32\n"
     ]
    }
   ],
   "source": [
    "np.info(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f14d870a2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f14d870a2f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimization=[tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "open(\"linear.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[777.65106]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([78]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
